{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for preparing data for spine labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sag_path = '/home/donal/PhD/initial_spines/CT_models/data/all_verts/'\n",
    "coronal_projections_path = '../images_coronal/all_projections/'\n",
    "sag_projections_path = '../images_sagittal/all_projections/'\n",
    "data_list = '/home/donal/PhD/initial_spines/CT_models/data_lists/data_list_all_forviewing.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_verts = ['T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'L1', 'L2', 'L3', 'L4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for loading/checking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centre(pts_file):\n",
    "    \"\"\"\n",
    "    Find centre point of vertebral body from sagittal annotations\n",
    "    \"\"\"\n",
    "    with open(pts_file, 'r') as f:\n",
    "        text = f.readlines()\n",
    "        lines = [line.strip() for line in text]\n",
    "        start = lines.index('{')\n",
    "        end = lines.index('}')\n",
    "        points = lines[start+1:end]\n",
    "        top_y = [float(line.split(' ')[-1]) for line in points[0:10]]\n",
    "        bot_y = [float(line.split(' ')[-1]) for line in points[19:29]]\n",
    "        top_y.extend(bot_y)\n",
    "    return np.mean(top_y)\n",
    "\n",
    "def get_mask(pts_file):\n",
    "    \"\"\"\n",
    "    Get all annotations for every level\n",
    "    \"\"\"\n",
    "    with open(pts_file, 'r') as f:\n",
    "        text = f.readlines()\n",
    "        lines = [line.strip() for line in text]\n",
    "        start = lines.index('{')\n",
    "        end = lines.index('}')\n",
    "        x = [float(x.split(' ')[0]) for x in lines[start+1:end]]\n",
    "        y = [float(x.split(' ')[1]) for x in lines[start+1:end]]\n",
    "        points = (x, y)\n",
    "    return points\n",
    "    \n",
    "\n",
    "\n",
    "def check_rejects(pts_dict):\n",
    "    # Check for overlapping annotations\n",
    "    reject_list = []\n",
    "    for key, val in pts_dict.items():\n",
    "        for vert, elem in val.items():\n",
    "            # Find nearest centre point\n",
    "            coords = list(val.values())\n",
    "            coords.remove(elem)\n",
    "            zipped_coords = list(zip(*coords))\n",
    "            near_neigh = sorted(\n",
    "                list(zipped_coords[-1]), key=lambda x: abs(x-elem[-1]))[0]\n",
    "            # Find distance between centroid for vert and nearest neighbour\n",
    "            dist = int(abs(elem[-1]-near_neigh))\n",
    "            if dist == 0:\n",
    "                print(key, vert)\n",
    "                reject_list.append(key)\n",
    "                continue\n",
    "    return reject_list\n",
    "\n",
    "def find_x_val(coronal_pts, tgt):\n",
    "    \"\"\"\n",
    "    Find x val in coronal projection by finding coronal midline\n",
    "    \"\"\"\n",
    "    with open(coronal_pts, 'r') as f:\n",
    "        text = f.readlines()\n",
    "        lines = [line.strip() for line in text]\n",
    "        start = lines.index('{')\n",
    "        end = lines.index('}')\n",
    "        points = lines[start+1:end]\n",
    "        y_vals = [float(line.split(' ')[-1]) for line in points]\n",
    "        x_vals = [float(line.split(' ')[0]) for line in points]\n",
    "    # Get top 2 closest values to sag. midpoint (y-axis)\n",
    "    mini = sorted(y_vals, key=lambda t: abs(t-tgt))[:2]\n",
    "    idx = [y_vals.index(val) for val in mini]\n",
    "    select_x = np.asarray(x_vals)[idx]\n",
    "    del_y = mini[0] - mini[1]\n",
    "    del_x = select_x[0]-select_x[1]\n",
    "    if del_x == 0:\n",
    "        return select_x[0]\n",
    "    else:\n",
    "        slope = del_y/del_x\n",
    "        b = mini[0]-slope*select_x[0]\n",
    "    #y=mx+b => x = (y-b)/m\n",
    "    return (tgt-b)/slope\n",
    "\n",
    "def get_id():\n",
    "    \"\"\"\n",
    "    Collect paths to point files, in a dict\n",
    "    \"\"\"\n",
    "    sag_files = [file for file in os.listdir(sag_path)]\n",
    "    pts_files = {}\n",
    "    with open(data_list, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        start = lines.index('{')\n",
    "        end = lines.index('}')\n",
    "        info_list = lines[start+1:end]\n",
    "        for line in info_list:\n",
    "            pts, img = line.split(':')\n",
    "            if '_Sag_' in pts:\n",
    "                id_ = pts.split('.')[0].split('_Sag_')[0]\n",
    "            else:\n",
    "                id_ = pts.split('.')[0].split('_midline_')[0]\n",
    "\n",
    "            matched_mip = [filename for filename in os.listdir(\n",
    "                coronal_projections_path) if id_ in filename]\n",
    "            for file in matched_mip:\n",
    "                id_ = file.split('.npy')[0]\n",
    "                vert_list = list(\n",
    "                    filter(lambda x: f'{id_}_midline' in x, sag_files))\n",
    "                if not vert_list:\n",
    "                    continue\n",
    "                else:\n",
    "                    pts_files[id_] = vert_list\n",
    "    return pts_files\n",
    "\n",
    "def get_points(pts_files):\n",
    "    \"\"\"\n",
    "     Get coordinates of each vertebral centre point\n",
    "    \"\"\"\n",
    "    mask_dict = {} #All landmark annotations for making a mask of vert.\n",
    "    pts_dict = {}\n",
    "    for key, val in pts_files.items():\n",
    "        name = f'{key}_kj'\n",
    "        pts_dict[name] = {}\n",
    "        \n",
    "        mask_dict[name] = {}\n",
    "        for elem in val:\n",
    "            # Find name of vertebra\n",
    "            name_split = re.findall('[0-9a-zA-Z][^A-Z]*',\n",
    "                                    os.path.splitext(elem)[0])\n",
    "            vert = name_split[-1].split('_')[0]\n",
    "            # Get y-value of centre-point on saggital\n",
    "            coronal_filename = f'{name}.jpg.pts'\n",
    "            centre_point = get_centre(os.path.join(sag_path, elem))\n",
    "            \n",
    "            # Get all landmark point annotations\n",
    "            points = get_mask(os.path.join(sag_path, elem))\n",
    "            mask_dict[name][vert] = points\n",
    "            # Get x-value\n",
    "            x_val = find_x_val(\n",
    "                f'/home/donal/PhD/initial_spines/CT_models/MIP/data/points/{coronal_filename}', centre_point)\n",
    "            pts_dict[name][vert] = (x_val, centre_point)\n",
    "\n",
    "    return pts_dict, mask_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 353 points files w/ matching MIP\n",
      "Found 353 vertebral annotations.\n",
      "Found 353 full vert. annotations.\n"
     ]
    }
   ],
   "source": [
    "pts_files = get_id()\n",
    "print(f'Found {len(list(pts_files.keys()))} points files w/ matching MIP')\n",
    "pts_dict, mask_dict = get_points(pts_files)\n",
    "print(f'Found {len(list(pts_dict.keys()))} vertebral annotations.')\n",
    "print(f'Found {len(list(mask_dict.keys()))} full vert. annotations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 rejects.\n"
     ]
    }
   ],
   "source": [
    " vert_list = []\n",
    "# Convert vertebra names to one-hot\n",
    "for val in pts_dict.values():\n",
    "    vert_list.extend(list(val.keys()))\n",
    "all_verts = list(np.unique(vert_list))\n",
    "enc = LabelBinarizer()\n",
    "enc.fit(all_verts)\n",
    "enc.classes_ = ordered_verts\n",
    "# Check for overlapping annotations\n",
    "reject_list = set(check_rejects(pts_dict))\n",
    "# Add key to rejects -- Can't remember why I chose to reject this one...\n",
    "reject_list.add('23_05_2014_153_Sag')\n",
    "print(f'Found {len(reject_list)} rejects.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read pixel info, collected when MIPs were performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum pixel size across dataset: 0.3125 mm\n",
      "                                              Size (mm)\n",
      "Name                                                   \n",
      "18_03_2015_20150318152008968_SRS00000_mip_WL   0.685000\n",
      "fr_580_LS_Sag_mip_WL                           0.781250\n",
      "29_05_2014_60_Sag_mip_WL                       0.781250\n",
      "fr_560_TS_Sag_mip_WL                           0.701172\n",
      "06_06_2014_502_LS_Sag_mip_WL                   0.800000\n"
     ]
    }
   ],
   "source": [
    " # Read pixel info\n",
    "pix_info = pd.read_csv(\n",
    "    '/home/donal/CT_volumes/pixel_size.csv', index_col='Name')\n",
    "min_pix = pix_info.loc[:, 'Size (mm)'].min()\n",
    "print(f'Minimum pixel size across dataset: {min_pix} mm')\n",
    "print(pix_info.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import scipy.stats as stats\n",
    "from scipy.special import softmax\n",
    "from scipy.spatial import ConvexHull "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_images(pts_dict, mask_dict, rejects, pix_info, outpath='./data/annotated_sanity/', plot=False):\n",
    "    # Prepare data for Train/Test/Val split\n",
    "    cor_info_csv = '../images_coronal/annotation_info.csv'\n",
    "    sag_info_csv = '../images_sagittal/annotation_info.csv'\n",
    "    clean_path = '/home/donal/PhD/initial_spines/CT_models/FCN/data/clean_data.npz'\n",
    "    clean_data = np.load(clean_path)\n",
    "    id_list = [''.join(elem) for elem in clean_data['id']]\n",
    "    cor_info = pd.read_csv(cor_info_csv)\n",
    "    sag_info = pd.read_csv(sag_info_csv)\n",
    "    for key, val in pts_dict.items():\n",
    "        print(key)\n",
    "        if key.split('_kj')[0] not in rejects:\n",
    "            key = key.split('_kj')[0]\n",
    "            # Check if ID is in clean_data (i.e annotation is ok)\n",
    "            if key not in id_list:\n",
    "                print('Issue with image, skipping...')\n",
    "                continue\n",
    "            out_img_path = os.path.join(outpath, f'{key}_annotated.tiff')\n",
    "            # Load projection images\n",
    "            cor_filename = os.path.join(coronal_projections_path, f'{key}.npy')\n",
    "            cor_img = np.load(cor_filename)\n",
    "            sag_filename = os.path.join(sag_projections_path, f'{key}.npy')\n",
    "            sag_img = np.load(sag_filename)\n",
    "\n",
    "            # Get info from pre-processing images\n",
    "            cor = cor_info.loc[cor_info['Name'] == key]\n",
    "            cor_pad = literal_eval(cor.iloc[0]['Padding'])\n",
    "            cor_scale = literal_eval(cor.iloc[0]['Pixel Scaling'])\n",
    "            # Load vert annotations\n",
    "            vert_dict = {}\n",
    "            annot_dict = {}\n",
    "            for vert, elem in val.items():\n",
    "                # Account for resampling to isotropic grid\n",
    "                x, y = (x*s for x, s in zip(elem, cor_scale))\n",
    "                y -= cor_pad[1]\n",
    "                vert_dict[vert] = y\n",
    "                \n",
    "                # Update vert. annotations\n",
    "                points = mask_dict[f'{key}_kj'][vert]\n",
    "                x = [(x*cor_scale[0])-cor_pad[0] for x in points[0]]\n",
    "                y = [(x*cor_scale[1])-cor_pad[1] for x in points[1]]\n",
    "                form_points = np.column_stack((x, y))\n",
    "    \n",
    "                annot_dict[vert] = form_points\n",
    "                break\n",
    "                \n",
    "            \n",
    "            # Convert y coordinate to heatmap\n",
    "            gt_holder = np.zeros((*cor_img.shape[:2], len(ordered_verts)), dtype=np.float32)\n",
    "            mask_holder = np.zeros((*cor_img.shape[:2], len(ordered_verts)+1), dtype=np.int16)\n",
    "            for vert, y in vert_dict.items():\n",
    "                channel = ordered_verts.index(vert)\n",
    "                x = np.linspace(0, 512, 512)\n",
    "                gauss = np.array(stats.norm.pdf(x, y, 10))[..., np.newaxis]\n",
    "                norm_gauss = gauss\n",
    "                \n",
    "                annot_dict[key]\n",
    "                \n",
    "                tmp = np.tile(norm_gauss, (1, 512)).astype(np.float32)\n",
    "                gt_holder[..., channel] = tmp\n",
    "                mask_holder[..., channel+1] = tmp_mask\n",
    "            \n",
    "            # Save heatmap to folder\n",
    "            np.save(f'../data/heatmaps/{key}.npy', gt_holder)\n",
    "            np.save(f'../data/masks/{key}.npy', mask_holder)\n",
    "            \n",
    "            with open(f'../data/coordinates/{key}.csv', 'w') as f:\n",
    "                wrt = csv.writer(f, dialect='excel')\n",
    "                wrt.writerow(['Level', 'Coordinate'])\n",
    "                for vert, coord in vert_dict.items():\n",
    "                    wrt.writerow([vert, coord])\n",
    "\n",
    "            if plot:\n",
    "                x = np.linspace(0, 512, num=512)\n",
    "                fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "                ax = axes.ravel()\n",
    "                ax[0].axis('off')\n",
    "                ax[1].axis('off')\n",
    "                #ax[2].axis('off')\n",
    "                ax[3].axis('off')\n",
    "                plt.tight_layout()\n",
    "                ax[0].imshow(cor_img)\n",
    "                ax[1].imshow(sag_img)\n",
    "                ax[3].imshow(sag_img)\n",
    "                ax[3].imshow(gt_holder[..., 12], alpha=0.5)\n",
    "                #ax[3].imshow(np.argmax(mask_holder, axis=-1), alpha=0.5)\n",
    "                for vert, y in vert_dict.items():\n",
    "                    ax[0].axhline(y, linewidth=2, c='y')\n",
    "                    ax[0].text(0, y-5, vert, color='w')\n",
    "                    ax[1].axhline(y, linewidth=2, c='y')\n",
    "                    ax[1].text(0, y-5, vert, color='w')\n",
    "                    channel = ordered_verts.index(vert)\n",
    "                    heatmap = gt_holder[..., channel]\n",
    "                    ax[2].plot(x, heatmap, label=vert)\n",
    "\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_06_2014_363_Sag_kj\n",
      "(33, 2)\n",
      "[[542.1588504  142.601025  ]\n",
      " [548.5873176  142.40428203]\n",
      " [555.0729864  142.56401016]\n",
      " [561.1820424  142.44425234]\n",
      " [567.7905384  142.39008438]\n",
      " [574.80336    142.00224844]\n",
      " [581.73024    141.55588125]\n",
      " [587.9213016  140.80150078]\n",
      " [593.8603632  140.25741172]\n",
      " [599.3403816  139.82648438]\n",
      " [605.0075328  139.54399766]\n",
      " [610.2588     140.83787813]\n",
      " [611.9139048  143.3540625 ]\n",
      " [613.1015616  146.11321094]\n",
      " [614.63424    148.81431406]\n",
      " [616.4335704  151.44985313]\n",
      " [618.6180912  153.95324219]\n",
      " [620.869812   156.24672734]\n",
      " [622.4711784  158.29057109]\n",
      " [619.69632    160.00254688]\n",
      " [613.8632448  160.55816953]\n",
      " [606.9259632  160.725625  ]\n",
      " [600.0156384  160.80930781]\n",
      " [593.1123408  160.96578125]\n",
      " [585.743592   161.2567125 ]\n",
      " [577.8627312  161.76050313]\n",
      " [570.5495352  162.53314609]\n",
      " [564.338196   163.36125703]\n",
      " [558.9807504  164.27766641]\n",
      " [553.2356328  164.62727188]\n",
      " [550.23186    163.4414375 ]\n",
      " [549.4949352  161.67654375]\n",
      " [548.1788448  159.29707891]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'01_06_2014_363_Sag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-cc2f06fc3b81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprep_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreject_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpix_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-ed22a009c8f6>\u001b[0m in \u001b[0;36mprep_images\u001b[0;34m(pts_dict, mask_dict, rejects, pix_info, outpath, plot)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mnorm_gauss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgauss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mannot_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_gauss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '01_06_2014_363_Sag'"
     ]
    }
   ],
   "source": [
    "prep_images(pts_dict, mask_dict, reject_list, pix_info, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
